import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(linewidth=np.nan)
import os
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from torch.optim.lr_scheduler import StepLR

# 0) Load data
# Defining class for loading data
class IsingDataset(Dataset):
    def __init__(self, img_dir, label_dir):
        self.img_labels = torch.load(label_dir)
        self.img_data = torch.load(img_dir)
        self.transform = None

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        image = self.img_data[idx]
        label = self.img_labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

# Defining function for calculating accuracy
def test_accuracy(net, testloader, diff_array=None, device="cpu"):
    correct = 0
    total = 0
    with torch.no_grad():
        if diff_array == None: # if diff_array is not given, then return accuracy and do not store diff_array
            for i, (images, labels) in enumerate(testloader):
                images, labels = images.to(device), (labels).to(device)
                outputs = net(images)
                predicted = outputs
                total += labels.size(0)
                for j, (k, l) in enumerate(zip(predicted,labels)):
                    diff = abs(k-l).item()
                    correct += (diff < 0.01*k.item())
        else: # if diff_array is given, then return accuracy and store diff_array
            for i, (images, labels) in enumerate(testloader):
                images, labels = images.to(device), (labels).to(device)
                outputs = net(images)
                predicted = outputs
                total += labels.size(0)
                for j, (k, l) in enumerate(zip(predicted,labels)):
                    diff = abs(k-l).item()
                    correct += (diff < 0.01*k.item())
                    diff_array[i*test_batch_size+j, 0] = l
                    diff_array[i*test_batch_size+j, 1] = diff

    return correct / total, diff_array

# Defining function for loading data
def load_data(grid_dir="./grid_data", committor_dir="./committor_data"):    
    dataset = IsingDataset(grid_dir, committor_dir)

    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])

    return trainset, testset

# 1) Model
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        # convolutional layers
        self.conv1_1 = nn.Conv2d(1, 8, 5, padding=[2,2], padding_mode='circular')
        self.conv1_2 = nn.Conv2d(8, 8, 5, 2, padding=[2,2], padding_mode='circular')
        self.conv2_1 = nn.Conv2d(8, 16, 7, padding=[3,3], padding_mode='circular')
        self.conv2_2 = nn.Conv2d(16, 16, 7, 2, padding=[3,3], padding_mode='circular')
        self.conv3_1 = nn.Conv2d(16, 32, 11, padding=[5,5], padding_mode='circular')
        self.conv3_2 = nn.Conv2d(32, 32, 11, 2, padding=[5,5], padding_mode='circular')
        
        # pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        # fully connected layer
        self.fc1 = nn.Linear(32*8*8, 516)
        self.fc2 = nn.Linear(516, 516)
        self.fc3 = nn.Linear(516, 1)
        # extra layers
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # convolutional layers
        # layer 1
        x = self.conv1_1(x)
        x = F.softplus(self.conv1_2(x))
        # layer 2
        x = self.conv2_1(x)
        x = F.softplus(self.conv2_2(x))
        # layer 3
        x = self.conv3_1(x)
        x = F.softplus(self.conv3_2(x))

        # fully connected layer
        x = x.view(-1, 32*8*8)
        x = F.dropout(x, p=0, training=self.training)
        x = F.softplus(self.fc1(x))
        x = F.softplus(self.fc2(x))
        x = self.fc3(x)
        return x

# Device configuration and setting up network
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
net = ConvNet().to(device)
device_cpu = torch.device('cpu')
net_cpu = ConvNet().to(device_cpu)  

# Printing number of parameters
total_params = sum(p.numel() for p in net.parameters())
print("Parameters: ", total_params)
#exit()

# 2) Hyper-parameters, loss and optimizer, data loading
num_epochs = 500
learning_rate = 0.001
weight_decay = 0.0001
train_batch_size = 10
test_batch_size = 1

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adagrad(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
scheduler = StepLR(optimizer, step_size=1, gamma=0.5)

# Loading data
trainset, testset = load_data()
trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size=train_batch_size)
testloader = torch.utils.data.DataLoader(testset, shuffle=False, num_workers=2, batch_size=test_batch_size)

# Data augmentation
transform = transforms.RandomApply(torch.nn.ModuleList([transforms.transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip()]), p=0.50)

# 3) Training loop
PATH = './model.pth'

# Loading model
load = 0
if load == 1:
    checkpoint = torch.load(PATH)
    net.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    net.train()
    print("Loaded NN")

# Initializing counters
accuracy = 0
revert_to_checkpoint = 0
revert_limit = 0
epoch = 0

# Running training loop
run = 1
if run == 1:
    print('Beginning Training Loop')
    n_total_steps = np.int32(len(trainset)/train_batch_size)
    correct = 0
    while (1):
        epoch += 1
        if epoch >= num_epochs:
            break
        if revert_limit == 10: # Reset learning rate if no improvement after 10 checkpoint revertions
            revert_limit = 0
            optimizer.param_groups[0]["lr"] = learning_rate
            print("Revertion limit reached, resetting learning rate to initial value.")
        if accuracy < 0.05 and epoch < 50: # Resetting optimizer if accuracy is too low
            epoch = 0
            optimizer = torch.optim.Adagrad(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
            print("Accuracy too low, resetting optimizer and restarting from epoch 1.")
        for i, (images, labels) in enumerate(trainloader):
            #images = torch.roll(transform(images), shifts=(torch.randint(64, [1]).item(), torch.randint(64, [1]).item()), dims=(-1, -2))
            images = torch.roll(images, shifts=(torch.randint(64, [1]).item(), torch.randint(64, [1]).item()), dims=(-1, -2))
            images = images.to(device)
            labels = (labels).to(device)
            outputs = net(images)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        revert_to_checkpoint += 1 # Incrementing checkpoint counter
        test_loss = loss.item()
        # Test accuracy
        net_cpu.load_state_dict(net.state_dict())
        net_cpu.eval()
        test_acc = test_accuracy(net_cpu, testloader)

        # Saving checkpoint
        if (accuracy < test_acc):
            revert_to_checkpoint = 0
            revert_limit = 0
            accuracy = test_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': net.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'accuracy': accuracy,
                }, PATH)
        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {test_loss:.4f}, Current Accuracy: {test_acc*100:.1f}%, Best Accuracy: {accuracy*100:.1f}%')
        # If no improvement after 25 epochs, revert to previous checkpoint
        if revert_to_checkpoint == 25:
            revert_to_checkpoint = 0
            revert_limit += 1
            checkpoint = torch.load(PATH)
            net.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            epoch = checkpoint['epoch']
            accuracy = np.round(0.99*accuracy, decimals=3)
            scheduler.step()
            print("Checkpoint loaded. Starting from epoch: ", epoch)
        
        
else:
    net.load_state_dict(torch.load(PATH))
    net.eval()
    diff_array = np.zeros([len(testset),2])
    print('Testing')
    test_acc = test_accuracy(net, testset, diff_array=diff_array)
    print (f'Accuracy: {test_acc*100:.1f}%')